import os
from dotenv import load_dotenv
from langchain_ollama import ChatOllama
from langchain_core.messages import AIMessage
from langchain_core.prompts import ChatPromptTemplate

# 1. åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
MODEL_NAME = os.getenv("OLLAMA_MODEL", "llama3:8b")
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")

# 2. åˆå§‹åŒ– ChatOllamaï¼ˆä¿ç•™ä½ çš„åŸå§‹é…ç½®ï¼Œstreaming=Trueï¼‰
llm = ChatOllama(
    model=MODEL_NAME,
    base_url=OLLAMA_BASE_URL,
    temperature=0.7,
    timeout=30.0,
    streaming=True,
      # ä¿ç•™OllamaåŸç”Ÿæ ¼å¼ï¼Œä¿è¯æ¨¡å‹å“åº”æ­£å¸¸
)

# 3. å®šä¹‰ ChatPromptTemplateï¼ˆæ ¸å¿ƒï¼šå°è£…ç³»ç»Ÿæç¤ºè¯+ç”¨æˆ·è¾“å…¥å ä½ç¬¦ï¼‰
# æ¨¡æ¿ç»“æ„ï¼šSystem Messageï¼ˆå›ºå®šï¼‰ + Human Messageï¼ˆåŠ¨æ€å ä½ç¬¦ï¼‰
# chat_template = ChatPromptTemplate.from_messages([
#     # å›ºå®šç³»ç»Ÿæç¤ºè¯ï¼šçº¦æŸæ¨¡å‹è¡Œä¸ºï¼Œå¯å¤ç”¨
#     ("system", "ä½ æ˜¯ä¸€ä¸ªæ•°å­¦è®¡ç®—åŠ©æ‰‹ï¼Œä¸¥æ ¼éµå®ˆï¼š1. åªè¿”å›è®¡ç®—ç»“æœçš„æ•°å­—ï¼›2. ä¸å¯’æš„ã€ä¸è§£é‡Šã€æ— å¤šä½™æ–‡å­—ï¼›3. ç²¾å‡†è®¡ç®—ï¼Œä¸å‡ºé”™ã€‚"),
#     # åŠ¨æ€ç”¨æˆ·è¾“å…¥ï¼šç”¨ {user_input} ä½œä¸ºå ä½ç¬¦ï¼Œåç»­å¡«å……å…·ä½“é—®é¢˜
#     ("human", "{user_input}")
# ])

# ==================== æ–¹å¼1ï¼šå•æ¬¡å¯¹è¯ï¼ˆæ¨¡æ¿å¡«å……+æ¨¡å‹è°ƒç”¨ï¼‰====================

chat_template = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªå°è¯´å®¶ã€‚"),
    ("human", "{user_input}")
])
# ==================== æ–¹å¼2ï¼šå¤šè½®å¯¹è¯ï¼ˆæ¨¡æ¿å¤ç”¨+å†…å­˜è®°å¿†ï¼‰====================
# 4. æ ¸å¿ƒï¼šç»ˆç«¯æŒç»­äº¤äº’å¼å¯¹è¯ï¼ˆæµå¼è¾“å‡ºï¼‰
def receive_stream_output():
    # 4.1 å¡«å……æ¨¡æ¿ï¼Œç”Ÿæˆåˆæ³•è¾“å…¥
    filled_messages = chat_template.format_messages(user_input="ä½ å¥½")

    # 4.2 è°ƒç”¨ stream()ï¼Œè·å–è¿­ä»£å™¨ï¼ˆIterator[AIMessageChunk]ï¼‰
    stream_iterator = llm.stream(filled_messages)

    # 4.3 è¿­ä»£è¿­ä»£å™¨ï¼Œæå–æœ‰æ•ˆå†…å®¹ï¼ˆè§£å†³ç©ºç™½å›å¤çš„å…³é”®ï¼‰
    print("ğŸ¤– AI å“åº”ï¼š", end="", flush=True)  # ä¿æŒæ‰“å­—æœºæ•ˆæœï¼Œä¸æ¢è¡Œ
    full_response = ""  # å¯é€‰ï¼Œç”¨äºæ‹¼æ¥å®Œæ•´å“åº”ç»“æœ

    for chunk in stream_iterator:
        # å…³é”®ç»†èŠ‚ 1ï¼šæå– chunk.content å­—æ®µï¼ˆè¿™æ‰æ˜¯æœ‰æ•ˆå“åº”æ–‡æœ¬ï¼‰
        valid_content = chunk.content

        # å…³é”®ç»†èŠ‚ 2ï¼šè¿‡æ»¤ç©ºå†…å®¹å—ï¼ˆéƒ¨åˆ†æ¨¡å‹ä¼šè¿”å›ç©ºå—ï¼Œé¿å…æ— æ•ˆæ‰“å°ï¼‰
        if not valid_content:
            continue

        # å…³é”®ç»†èŠ‚ 3ï¼šæ‰“å°æœ‰æ•ˆå†…å®¹ï¼Œä¿æŒæ‰“å­—æœºæ•ˆæœ
        print(valid_content, end="", flush=True)

        # å¯é€‰ï¼šæ‹¼æ¥å®Œæ•´å“åº”ï¼Œç”¨äºåç»­å­˜å‚¨/å¤ç”¨
        full_response += valid_content

    print(f"\n\nâœ… å®Œæ•´å“åº”ç»“æœï¼ˆå¯å­˜å‚¨ï¼‰ï¼š{full_response}")
if __name__ == "__main__":
    receive_stream_output()